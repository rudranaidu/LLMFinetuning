 1. transformers
ğŸ“¦ Hugging Face Transformers Library

pip install transformers

ğŸ” Why you need it:
  Core library for loading pre-trained models and tokenizers (like mistral, llama, falcon, etc.)
  Used for training, inference, and text generation
  Provides Trainer and TrainingArguments classes
  Supports models in both encoder (BERT-style) and decoder (GPT-style) formats

Used for:
Loading models: AutoModelForCausalLM, AutoTokenizer
Defining training pipeline
Model inference and text generation after fine-tuning

âœ… 2. datasets
ğŸ“¦ Hugging Face Datasets Library

pip install datasets
ğŸ” Why you need it:
For loading, preprocessing, and managing datasets
Supports large-scale datasets in streaming mode
Makes it easy to load formats like JSON, CSV, Parquet, etc.

ğŸ› ï¸ Used for:
Loading datasets: load_dataset("alpaca")
Splitting into train/test
Tokenizing and batching for model training

âœ… 3. accelerate
ğŸ“¦ Training and hardware acceleration manager

pip install accelerate
ğŸ” Why you need it:
Seamlessly handles training across:
  CPU
  GPU
  Multi-GPU
  FP16 (mixed precision)
Simplifies launching training without manually managing CUDA devices

ğŸ› ï¸ Used for:
Running training loops efficiently
Works under the hood of transformers.Trainer
Required when you use bitsandbytes or mixed precision training

âœ… 4. peft
ğŸ“¦ Parameter-Efficient Fine-Tuning library by Hugging Face
pip install peft

ğŸ” Why you need it:
Implements LoRA, Prefix Tuning, IA3, etc.
Fine-tunes only a small number of weights instead of full model
Saves memory, reduces compute needs â€” ideal for local setup
ğŸ› ï¸ Used for:
Applying LoRA to models:

from peft import get_peft_model, LoraConfig
Integrates with Hugging Face Trainer

âœ… 5. bitsandbytes
ğŸ“¦ 8-bit & 4-bit quantization backend

pip install bitsandbytes
ğŸ” Why you need it:
Allows loading large models (like mistral-7b) in 8-bit or 4-bit
Saves VRAM/GPU memory
Works with transformers and peft

ğŸ› ï¸ Used for:
Loading model in quantized format:

model = AutoModelForCausalLM.from_pretrained("mistral", load_in_8bit=True)
âš ï¸ Requires GPU with CUDA. If using CPU, this wonâ€™t help.

âœ… 6. loralib
ğŸ“¦ Core library for LoRA math

pip install loralib
ğŸ” Why you need it:
Backend dependency used by peft for LoRA implementation
Contains low-rank decomposition logic and matrix operations

ğŸ› ï¸ Used for:
Under the hood by peft during training and inference

âœ… 7. sentencepiece
ğŸ“¦ Tokenizer for sentence-level models (like LLaMA, T5)

pip install sentencepiece
ğŸ” Why you need it:
Some LLMs (e.g., LLaMA, T5, Mistral) use SentencePiece-based tokenizers instead of WordPiece or Byte-level BPE

ğŸ› ï¸ Used for:
Tokenizing inputs correctly before training or inference

Required for models like:
mistralai/Mistral-7B
facebook/mbart
meta-llama/Llama-2-7b-hf

âœ… 8. trl (Transformers Reinforcement Learning)
ğŸ“¦ Training library with SFTTrainer, PPO, DPO

pip install trl
ğŸ” Why you need it:
Provides custom SFTTrainer (Supervised Fine-Tuning Trainer) for instruction-tuning
Supports Reinforcement Learning from Human Feedback (RLHF)
Compatible with peft, transformers, and datasets

ğŸ› ï¸ Used for:
Simple training loop:

from trl import SFTTrainer
Often preferred over vanilla transformers.Trainer for SFT
